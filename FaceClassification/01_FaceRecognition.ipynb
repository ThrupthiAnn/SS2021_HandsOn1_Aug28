{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "01_FaceRecognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Biometrics/01_FaceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfyuh7lkdJl5"
      },
      "source": [
        "## Face Recognition\n",
        "\n",
        "In this tutorial, we are going to use the pre-trained VGG-Face network [1] to recognize faces of celebrities. The VGG-Face is a deep-CNN which has been trained using Softmax loss to recognize faces of 2,622 celebrity identities. We will use face images (that have been downloaded from the internet) of a random subset of 10  celebrities, out of the 2,622."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:13:26.434028Z",
          "iopub.status.busy": "2021-08-20T07:13:26.433639Z",
          "iopub.status.idle": "2021-08-20T07:13:27.233159Z",
          "shell.execute_reply": "2021-08-20T07:13:27.232663Z",
          "shell.execute_reply.started": "2021-08-20T07:13:26.434006Z"
        },
        "tags": [],
        "id": "a-9L0l08dJl-"
      },
      "source": [
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "import torchvision.models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline \n",
        "plt.ion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XldHDo_Pd0p2"
      },
      "source": [
        "# download all the data for this notebook.\n",
        "# Warning! This data is hosted on a personal Google drive and may be taken off at any time. \n",
        "#Also, the file is quite big, so please have patience!\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aReZgWlWfJef"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:30:13.239776Z",
          "iopub.status.busy": "2021-08-20T07:30:13.239574Z",
          "iopub.status.idle": "2021-08-20T07:30:13.243094Z",
          "shell.execute_reply": "2021-08-20T07:30:13.242603Z",
          "shell.execute_reply.started": "2021-08-20T07:30:13.239754Z"
        },
        "tags": [],
        "id": "mwCl5RhCdJmA"
      },
      "source": [
        "root = 'data'\n",
        "eroot = os.path.join(root, 'Experiment_1')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T06:38:52.402026Z",
          "iopub.status.busy": "2021-08-20T06:38:52.401800Z",
          "iopub.status.idle": "2021-08-20T06:38:52.406792Z",
          "shell.execute_reply": "2021-08-20T06:38:52.406313Z",
          "shell.execute_reply.started": "2021-08-20T06:38:52.402001Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "ETaygI-QdJmC"
      },
      "source": [
        "def getNameList(filePath):\n",
        "    names = []\n",
        "    with open(filePath) as f:\n",
        "        names = [ line.strip() for line in f ]\n",
        "    return names\n",
        "\n",
        "idNames = getNameList(os.path.join(root, \"names.txt\"))\n",
        "print(\"Number of identities = \", len(idNames))\n",
        "print (\"List of the first 5 identities = \", idNames[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsa-90s8dJmE"
      },
      "source": [
        "Now, let us take a quick look at some of the face images in our dataset. For the convenience of the participants, we have packaged the image names of all the 10 face images in the form of a .t7 data file that can be loaded onto our PyTorch scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:18:10.160855Z",
          "iopub.status.busy": "2021-08-20T07:18:10.160656Z",
          "iopub.status.idle": "2021-08-20T07:18:10.164496Z",
          "shell.execute_reply": "2021-08-20T07:18:10.164016Z",
          "shell.execute_reply.started": "2021-08-20T07:18:10.160833Z"
        },
        "tags": [],
        "id": "8_RnZonFdJmE"
      },
      "source": [
        "dataset = glob.glob(os.path.join(eroot, '*.*'))\n",
        "for ii in dataset:\n",
        "\tprint(ii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zELnWxCadJmF"
      },
      "source": [
        "Let us take the first image in our dataset and visualize what it looks like. Feel free to modify the code if you wish to look at more face images from the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:05:21.835541Z",
          "iopub.status.busy": "2021-08-20T07:05:21.835339Z",
          "iopub.status.idle": "2021-08-20T07:05:22.139737Z",
          "shell.execute_reply": "2021-08-20T07:05:22.129062Z",
          "shell.execute_reply.started": "2021-08-20T07:05:21.835519Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6LopvVSfdJmH"
      },
      "source": [
        "dispImg = mpimg.imread(dataset[0])\n",
        "imgplot = plt.imshow(dispImg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0xhHUxJdJmI"
      },
      "source": [
        "Before sending the images through the network, we have to perform certain pre-processing steps to make our input image \"suitable\" for the network. Each CNN has a fixed size of input images that it can accept. The VGG-Face netowrk architecture has been trained on 224 $\\times$ 224 RGB images. Therefore, any image that we send as an input must be of 224 $\\times$ 224 size.\n",
        "\n",
        "So, can we simply perform a resize operation on our input image and convert it to the appropriate size? Performing a simple resize operation on arbitrary sized input images may result in a change in the aspect ratio, thereby making the face image look distorted. To avoid that, we perform the following sequence of operations -- \n",
        "\n",
        "1. Resize the image such that the smaller dimension (out of height and width) is 256 and the aspect ratio remains the same.\n",
        "2. Crop a 224 $\\times$ 224 region from the center of the resized image.\n",
        "\n",
        "We will perform the above steps using OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:07:38.923238Z",
          "iopub.status.busy": "2021-08-20T07:07:38.922859Z",
          "iopub.status.idle": "2021-08-20T07:07:38.933157Z",
          "shell.execute_reply": "2021-08-20T07:07:38.932707Z",
          "shell.execute_reply.started": "2021-08-20T07:07:38.923214Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "IWKzt04adJmJ"
      },
      "source": [
        "# load the image using OpenCV\n",
        "inputImg = cv2.imread(dataset[0])\n",
        "\n",
        "# re-scale the smaller dim (among width, height) to refSize\n",
        "refSize, targetSize = 256, 224\n",
        "imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
        "if imgCols < imgRows: resizedImg = cv2.resize(inputImg, (refSize, int(refSize * imgRows / imgCols)))\n",
        "else: resizedImg = cv2.resize(inputImg, (int(refSize * imgCols / imgRows), refSize))\n",
        "    \n",
        "# center-crop\n",
        "oH, oW = targetSize, targetSize\n",
        "iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
        "anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
        "croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
        "\n",
        "print(croppedImg.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJaJs5vadJmK"
      },
      "source": [
        "As you can see, we have resized and cropped our input image to the correct dimensions.\n",
        "\n",
        "Moving on, the VGG-Face network expects the shape of the input image to be $3 \\times 224 \\times 224$, which means that the 1st dimension corresponds to the image channel. On the other hand, what we have is an input image whose shape is $224 \\times 224 \\times 3$ where the last dimension is for the channel. Therefore, in order to make the shapes of the input Tensors consistent, we perform the necessary conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:07:54.495719Z",
          "iopub.status.busy": "2021-08-20T07:07:54.495494Z",
          "iopub.status.idle": "2021-08-20T07:07:54.499753Z",
          "shell.execute_reply": "2021-08-20T07:07:54.499279Z",
          "shell.execute_reply.started": "2021-08-20T07:07:54.495697Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "Kcc9cObjdJmL"
      },
      "source": [
        "# convert shape from (height, width, 3) to (3, width, height)\n",
        "channel_1, channel_2, channel_3 = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
        "croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
        "croppedImg[0], croppedImg[1], croppedImg[2] = channel_1, channel_2, channel_3\n",
        "\n",
        "print(croppedImg.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnSRJbQDdJmM"
      },
      "source": [
        "As a final step, we perform mean subtraction. You might be aware that the input data that is used for training deep CNNs is mean subtracted (this is critical for the stabiliity of the training process). Similarly, while training the VGG-Face network, the per-channel mean of the training images was computed and the same mean value now has to be subtracted for any face image that is used to test the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:08:00.197624Z",
          "iopub.status.busy": "2021-08-20T07:08:00.197386Z",
          "iopub.status.idle": "2021-08-20T07:08:00.203992Z",
          "shell.execute_reply": "2021-08-20T07:08:00.203494Z",
          "shell.execute_reply.started": "2021-08-20T07:08:00.197602Z"
        },
        "tags": [],
        "id": "w23LLn63dJmO"
      },
      "source": [
        "# subtract training mean\n",
        "inputImg = inputImg.astype(float)\n",
        "trainingMean = [129.1863, 104.7624, 93.5940]\n",
        "for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sbWYlWEdJmP"
      },
      "source": [
        "Before we proceed, a quick recap. We have pre-processed our input face images by performing the following steps:\n",
        "1. Resize (to a slightly bigger size) while maintaining aspect ratio\n",
        "2. Crop a face patch from the center of the required dimensions\n",
        "3. Convert to an appropriate shape\n",
        "4. Subtract the training mean\n",
        "\n",
        "Moving forward, let us load the pre-trained network and see it's structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:30:35.493514Z",
          "iopub.status.busy": "2021-08-20T07:30:35.493285Z",
          "iopub.status.idle": "2021-08-20T07:30:38.565696Z",
          "shell.execute_reply": "2021-08-20T07:30:38.565084Z",
          "shell.execute_reply.started": "2021-08-20T07:30:35.493491Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "nQfDWhbKdJmQ"
      },
      "source": [
        "def loadVGGModel( filename):\n",
        "\tdat2 = torch.load(filename)\n",
        "\t# copy dictionary\n",
        "\tif str.split(list(dat2.keys())[0],'.')[0] == 'module':\n",
        "\t\tdat = {}\n",
        "\t\tfor key in dat2.keys():\n",
        "\t\t\tk = '.'.join(str.split(key,'.')[1:])\n",
        "\t\t\tdat[k] = dat2[key]\n",
        "\telse:\n",
        "\t\tdat = dat2\n",
        "\t\t\n",
        "\tn_classes = dat['classifier.6.bias'].shape[0]\n",
        "\tmodel = torchvision.models.vgg16(pretrained = False)\n",
        "\tlastlayer = torch.nn.Linear(in_features = model.classifier[-1].in_features, \\\n",
        "\t\t\t\t\t\t\t   out_features = n_classes, \\\n",
        "\t\t\t\t\t\t\t   bias = True)\n",
        "\tmodel.classifier[-1] = lastlayer\n",
        "\tmodel.load_state_dict(dat)\n",
        "\treturn model\n",
        "\n",
        "# load pre-trained VGG-Face network\n",
        "vggFace = loadVGGModel(os.path.join(root, 'VGGFace16.pth'))\n",
        "vggFace = vggFace.to(device)\n",
        "print(vggFace)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSbj9vA1dJmR"
      },
      "source": [
        "As you can see, there are 40 layers in total. The architecture can be divided into 5 Convolutional blocks followed by 2 fc layers and a classification layer. Each convolutional block consists of multiple Conv+ReLU layers followed by a pooling layer.\n",
        "\n",
        "Let us now send our pre-processed input image through the network i.e. we are going to do a forward pass through the pre-trained VGG-Face network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:30:48.674321Z",
          "iopub.status.busy": "2021-08-20T07:30:48.672429Z",
          "iopub.status.idle": "2021-08-20T07:30:48.688286Z",
          "shell.execute_reply": "2021-08-20T07:30:48.687805Z",
          "shell.execute_reply.started": "2021-08-20T07:30:48.674287Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "eutRFfGYdJmS"
      },
      "source": [
        "# forward pass\n",
        "img = torch.Tensor(croppedImg).unsqueeze(0)\n",
        "output = vggFace.forward(img.to(device))\n",
        "output = output.detach().cpu().numpy()\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb1u_JoudJmT"
      },
      "source": [
        "The network has returned a single row of 2,622 entries. These values are the normalized log probabilities of our input face image belonging to each of the 2,622 celebrity IDs in the training set. So, in order to know which ID is the most likely (as per the network), we figure out the one which has the maximum probability.\n",
        "\n",
        "To get the name of the ID from the index, we use the list of ID names that we had loaded at the start of the tutorial. As you can guess, the order of names in the list is important and serves as a mapping between the index number and the name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:30:52.315328Z",
          "iopub.status.busy": "2021-08-20T07:30:52.314945Z",
          "iopub.status.idle": "2021-08-20T07:30:52.320197Z",
          "shell.execute_reply": "2021-08-20T07:30:52.319638Z",
          "shell.execute_reply.started": "2021-08-20T07:30:52.315305Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "l2B1J_ghdJmT"
      },
      "source": [
        "print(\"ID = \", idNames[np.argmax(output)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcXzpqF9dJmV"
      },
      "source": [
        "What are the top-k predictions made by the network for our input image?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:30:54.087609Z",
          "iopub.status.busy": "2021-08-20T07:30:54.087233Z",
          "iopub.status.idle": "2021-08-20T07:30:54.091821Z",
          "shell.execute_reply": "2021-08-20T07:30:54.091252Z",
          "shell.execute_reply.started": "2021-08-20T07:30:54.087586Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "1HmsJpjVdJmV"
      },
      "source": [
        "# top-k predictions\n",
        "ind = output[0].argsort()[-5:][::-1]\n",
        "for idx in ind:\n",
        "    print(idNames[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS-JvglSdJmW"
      },
      "source": [
        "We have seen how to use the pre-trained net to make predictions (face recognition). The VGG-Face net can also be used as a fixed feature extractor for face images. We simply ignore the outputs of the classification layer and take the output of the last fc-layer instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T07:31:39.402027Z",
          "iopub.status.busy": "2021-08-20T07:31:39.401817Z",
          "iopub.status.idle": "2021-08-20T07:31:39.434104Z",
          "shell.execute_reply": "2021-08-20T07:31:39.433635Z",
          "shell.execute_reply.started": "2021-08-20T07:31:39.402006Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "HJLkfq4HdJmX"
      },
      "source": [
        "def extractfeatures(x):\n",
        "\t# replicate the forward function\n",
        "\tx = vggFace.features(x)\n",
        "\tx = vggFace.avgpool(x)\n",
        "\tx = torch.flatten(x, 1)\n",
        "\t# forward pass only through the first 4 layers of the classifier\n",
        "\tfor ii in range(4):\n",
        "\t\tx = vggFace.classifier[ii](x)\n",
        "\treturn x\n",
        "\t\n",
        "\n",
        "features = extractfeatures(img.to(device)).detach().cpu().numpy()\n",
        "print(features.shape)\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmVto3hqdJmY"
      },
      "source": [
        "\n",
        "The 4096-d face features that we obtain in this fashion can used for a variety of face-related tasks, as we will see in the subsequent tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sko9_a8BdJmY"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Are all the 10 images in the dataset classified correctly?\n",
        "2. Modify the code to support batch mode of operation -- get predictions (top-1 and top-k) for multiple face images at once.\n",
        "3. Along with the predicted ID name, also print the probability of prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEgn5uLwdJmZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}