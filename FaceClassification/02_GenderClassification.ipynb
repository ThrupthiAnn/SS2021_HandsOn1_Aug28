{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "02_GenderClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Biometrics/02_GenderClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T00:30:49.836423Z",
          "iopub.status.busy": "2021-08-21T00:30:49.836114Z",
          "iopub.status.idle": "2021-08-21T00:30:49.842029Z",
          "shell.execute_reply": "2021-08-21T00:30:49.841358Z",
          "shell.execute_reply.started": "2021-08-21T00:30:49.836351Z"
        },
        "id": "5931b040"
      },
      "source": [
        "### Gender Classification\n",
        "\n",
        "In this tutorial, we are going to fine-tune the pre-trained VGG-Face descriptors for the task of classifying the gender of a person from his/her face image. As we saw in the previous tutorial, the VGG-Face network has been trained to recognize 2,622 celebrity IDs. Also, we can ignore/chop-off the classification layer of the network and treat the output of the fc-layer as a representation for the input face. We are going to learn a new Softmax  classification layer on top of these fc layer features and train it to classify the gender of the input face image into one of the two classes -- \"male\" and \"female\". By the end of this tutorial, the participants would appreciiate the fact that by minimal amount of fine-tuning, the pre-trained VGG-Face representations can be made useful for a variety of tasks that are different from the original task that the representations were learnt from i.e. face recognition."
      ],
      "id": "5931b040"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:30.847012Z",
          "iopub.status.busy": "2021-08-21T03:03:30.846825Z",
          "iopub.status.idle": "2021-08-21T03:03:32.070797Z",
          "shell.execute_reply": "2021-08-21T03:03:32.070087Z",
          "shell.execute_reply.started": "2021-08-21T03:03:30.846962Z"
        },
        "tags": [],
        "id": "a3614dc8"
      },
      "source": [
        "import cv2, math\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline \n",
        "plt.ion()"
      ],
      "id": "a3614dc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ksLqDg_mgjSY"
      },
      "source": [
        "# download all the data for this notebook.\n",
        "# Warning! This data is hosted on a personal Google drive and may be taken off at any time. \n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip data.zip"
      ],
      "id": "ksLqDg_mgjSY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.072112Z",
          "iopub.status.busy": "2021-08-21T03:03:32.071949Z",
          "iopub.status.idle": "2021-08-21T03:03:32.673078Z",
          "shell.execute_reply": "2021-08-21T03:03:32.672336Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.072092Z"
        },
        "tags": [],
        "id": "1a2503b2"
      },
      "source": [
        "root = 'data'\n",
        "eroot = os.path.join(root, 'Experiment_2')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "id": "1a2503b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e24fd9"
      },
      "source": [
        "### Dataset\n",
        "We are going to use a subset of the CelebA dataset for our experiment. CelebA is a large-scale celebrity face attributes dataset. It consists of more than 200k celebrity face images, each having 40 binary face attribute annotations, with gender being one of them. We have selected a random subset of 200 face images -- 100 male and female faces each for the purpose of this experiment. Let us take a look at the distribution of the training and test splits."
      ],
      "id": "a5e24fd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.674715Z",
          "iopub.status.busy": "2021-08-21T03:03:32.674507Z",
          "iopub.status.idle": "2021-08-21T03:03:32.678901Z",
          "shell.execute_reply": "2021-08-21T03:03:32.678367Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.674691Z"
        },
        "tags": [],
        "id": "45c99985"
      },
      "source": [
        "with open(os.path.join(eroot, 'celeba-gender-dataset.json')) as f:\n",
        "\tdata = json.load(f)\n",
        "\n",
        "print('# images in trainset = ', data['train']['size'])\n",
        "print('# images in test set = ', data['test']['size'])\n"
      ],
      "id": "45c99985",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303d76b3"
      },
      "source": [
        "Let us take a look at some of the faces in the dataset, and their asosciated labels."
      ],
      "id": "303d76b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.679795Z",
          "iopub.status.busy": "2021-08-21T03:03:32.679633Z",
          "iopub.status.idle": "2021-08-21T03:03:32.826977Z",
          "shell.execute_reply": "2021-08-21T03:03:32.826426Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.679775Z"
        },
        "tags": [],
        "id": "c3f1424d"
      },
      "source": [
        "imgPath = os.path.join(eroot, data['train']['imgPaths'][0])\n",
        "imgLabel = data['train']['labels'][0]\n",
        "\n",
        "print(\"label = \", imgLabel)\n",
        "dispImg = mpimg.imread(imgPath)\n",
        "imgPlot = plt.imshow(dispImg)"
      ],
      "id": "c3f1424d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "439d5523"
      },
      "source": [
        "`label=0` stands for \"female\" whereas `label=1` stands for male. Let us also take a look at an example of a female face image in our training set."
      ],
      "id": "439d5523"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.828215Z",
          "iopub.status.busy": "2021-08-21T03:03:32.827982Z",
          "iopub.status.idle": "2021-08-21T03:03:32.965611Z",
          "shell.execute_reply": "2021-08-21T03:03:32.965135Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.828185Z"
        },
        "tags": [],
        "id": "19b7ee12"
      },
      "source": [
        "imgPath = os.path.join(eroot, data['train']['imgPaths'][100])\n",
        "imgLabel = data['train']['labels'][100]\n",
        "\n",
        "print(\"label = \", imgLabel)\n",
        "dispImg = mpimg.imread(imgPath)\n",
        "imgPlot = plt.imshow(dispImg)"
      ],
      "id": "19b7ee12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d8133c9"
      },
      "source": [
        "We have 160 training and 40 test images with each set having an equal distribution of male and female faces. Participants are encouraged to modify the code and view some of the different face images (and their asosciated labels) in the training as well as test sets.\n",
        "\n"
      ],
      "id": "7d8133c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "d4209e05"
      },
      "source": [
        "### Feature extraction\n",
        "\n",
        "We are using the VGG-Face pretrained network for feature extraction. Here, we load the network and create a function for extracting the features."
      ],
      "id": "d4209e05"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.966565Z",
          "iopub.status.busy": "2021-08-21T03:03:32.966396Z",
          "iopub.status.idle": "2021-08-21T03:03:32.972081Z",
          "shell.execute_reply": "2021-08-21T03:03:32.971342Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.966546Z"
        },
        "tags": [],
        "id": "b8cead93"
      },
      "source": [
        "def loadVGGModel( filename):\n",
        "\tdat2 = torch.load(filename)\n",
        "\t# copy dictionary\n",
        "\tif str.split(list(dat2.keys())[0],'.')[0] == 'module':\n",
        "\t\tdat = {}\n",
        "\t\tfor key in dat2.keys():\n",
        "\t\t\tk = '.'.join(str.split(key,'.')[1:])\n",
        "\t\t\tdat[k] = dat2[key]\n",
        "\telse:\n",
        "\t\tdat = dat2\n",
        "\t\t\n",
        "\tn_classes = dat['classifier.6.bias'].shape[0]\n",
        "\tmodel = torchvision.models.vgg16(pretrained = False)\n",
        "\tlastlayer = torch.nn.Linear(in_features = model.classifier[-1].in_features, \\\n",
        "\t\t\t\t\t\t\t   out_features = n_classes, \\\n",
        "\t\t\t\t\t\t\t   bias = True)\n",
        "\tmodel.classifier[-1] = lastlayer\n",
        "\tmodel.load_state_dict(dat)\n",
        "\treturn model\n",
        "\n",
        "def getFeature(x, model):\n",
        "\t# replicate the forward function\n",
        "\tx = model.features(x)\n",
        "\tx = model.avgpool(x)\n",
        "\tx = torch.flatten(x, 1)\n",
        "\t# forward pass only through the first 4 layers of the classifier\n",
        "\tfor ii in range(4):\n",
        "\t\tx = model.classifier[ii](x)\n",
        "\treturn x\n"
      ],
      "id": "b8cead93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:32.974117Z",
          "iopub.status.busy": "2021-08-21T03:03:32.973920Z",
          "iopub.status.idle": "2021-08-21T03:03:37.994170Z",
          "shell.execute_reply": "2021-08-21T03:03:37.993365Z",
          "shell.execute_reply.started": "2021-08-21T03:03:32.974098Z"
        },
        "tags": [],
        "id": "d14af1da"
      },
      "source": [
        "vggFace = loadVGGModel(os.path.join(root, 'VGGFace16.pth'))\n",
        "vggFace = vggFace.to(device)"
      ],
      "id": "d14af1da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de845911"
      },
      "source": [
        "#### Dataset class\n",
        "Now we will extend the pytorch dataset class for our custom dataset. We need to implement two functions: __len__ which gives the length of the dataset, and __getitem__ which returns an item of the dataset given an index. For every image of the dataset, we are using the pretrained features. But we are not extracting them here because it is more efficient to extract them in batch."
      ],
      "id": "de845911"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:37.996191Z",
          "iopub.status.busy": "2021-08-21T03:03:37.995954Z",
          "iopub.status.idle": "2021-08-21T03:03:38.004370Z",
          "shell.execute_reply": "2021-08-21T03:03:38.003918Z",
          "shell.execute_reply.started": "2021-08-21T03:03:37.996158Z"
        },
        "tags": [],
        "id": "5e13c632"
      },
      "source": [
        "def loadImage(imgPath):\n",
        "    inputImg = cv2.imread(imgPath)\n",
        "\n",
        "    # re-scale the smaller dim (among width, height) to refSize\n",
        "    refSize, targetSize = 256, 224\n",
        "    imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
        "    if imgCols < imgRows: resizedImg = cv2.resize(inputImg, (refSize, int(refSize * imgRows / imgCols)))\n",
        "    else: resizedImg = cv2.resize(inputImg, (int(refSize * imgCols / imgRows), refSize))\n",
        "\n",
        "    # center-crop\n",
        "    oH, oW = targetSize, targetSize\n",
        "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
        "    anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
        "    croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
        "\n",
        "    # convert shape from (height, width, 3) to (3, width, height)\n",
        "    channel_1, channel_2, channel_3 = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
        "    croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
        "    croppedImg[0], croppedImg[1], croppedImg[2] = channel_1, channel_2, channel_3\n",
        "\n",
        "    # subtract training mean\n",
        "    inputImg = inputImg.astype(float)\n",
        "    trainingMean = [129.1863, 104.7624, 93.5940]\n",
        "    for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]\n",
        "    return croppedImg\n",
        "\n",
        "class GenderDataset(Dataset):\n",
        "\tdef __init__(self, data):\n",
        "\t\tself.labels = data['labels']\n",
        "\t\tself.imgPaths = data['imgPaths']\n",
        "\t\tself.size = data['size']\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.size\n",
        "\t\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tlabel = self.labels[index]\n",
        "\t\timgpath = os.path.join(eroot, self.imgPaths[index])\n",
        "\t\timg = loadImage(imgpath)\n",
        "\t\treturn torch.tensor(img, dtype= torch.float32), label\n",
        "\t\t\n",
        "\t\t"
      ],
      "id": "5e13c632",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5062aa8"
      },
      "source": [
        "### Network Architecture\n",
        "Having defined the functions for pre-processing images and getting the pre-trained face descriptors, let us take a look at the network structure that we are going to train."
      ],
      "id": "e5062aa8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.005404Z",
          "iopub.status.busy": "2021-08-21T03:03:38.005216Z",
          "iopub.status.idle": "2021-08-21T03:03:38.009871Z",
          "shell.execute_reply": "2021-08-21T03:03:38.009451Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.005384Z"
        },
        "tags": [],
        "id": "f97a1946"
      },
      "source": [
        "# Network structure that we'll train for gender classification\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Net, self).__init__()\n",
        "\t\tself.fc1 = torch.nn.Linear(4096, 2048)\n",
        "\t\tself.fc2 = torch.nn.Linear(2048, 2)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = F.relu(self.fc1(x))\n",
        "\t\tx = self.fc2(x)\n",
        "\t\treturn x\n"
      ],
      "id": "f97a1946",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9239bb"
      },
      "source": [
        "The architecture is quite straightforward. We have two linear layers:  ```Linear(4096, 2048)``` and ```Linear(2048,2)```   which takes a 4096-d input and computes a vector that has two elements. Recall from the previous tutorial that the pre-trained VGG-Face descriptors are 4096-d in size. So, our fc-layers can take the pre-trained descriptors as input and return a pair of values which would be the (unnormalized) likelihoods of the input image belonging to each of the two classes -- \"male\" and \"female\".\n",
        "\n",
        "The first fc-layer is followed by a ```ReLu``` layer as the linearity. By training the parameters of the fc-layer to minimize the loss, our network will learn to map the pre-trained face descriptors to the correct gender class."
      ],
      "id": "7e9239bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71cc8af3"
      },
      "source": [
        "### Training\n",
        "Now that we have fixed the network architecture as well as the dataset, let us move on to training our network on the dataset. We are going to use a combination of `LogSoftMax` + `NLLLoss` in PyTorch to train the network. Let us take a look at the various steps involved.\n",
        "\n",
        "First of all, we fix the seed of the various random number generators that our code uses. Why do we need to do this? So that, all are experimental results are reproducible. Having fixed the seed, our network parameters will always be initialized with the same random values every time we run the experiments. Also, the sampling of the mini-batch during training will also follow the same order in different runs of the experiment. You can verify this fact by commenting out the 2 lines given below and comparing the value of the training loss in successive runs of the experiment."
      ],
      "id": "71cc8af3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.010848Z",
          "iopub.status.busy": "2021-08-21T03:03:38.010607Z",
          "iopub.status.idle": "2021-08-21T03:03:38.015726Z",
          "shell.execute_reply": "2021-08-21T03:03:38.015279Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.010828Z"
        },
        "tags": [],
        "id": "6017adeb"
      },
      "source": [
        "# fix the seeds of random number generators\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "id": "6017adeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0d4f26a"
      },
      "source": [
        "Next, we create the dataset. We also initialize our network architecture, loss module and certain other training parameters such as number of epochs to train and the batch size."
      ],
      "id": "d0d4f26a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.016626Z",
          "iopub.status.busy": "2021-08-21T03:03:38.016442Z",
          "iopub.status.idle": "2021-08-21T03:03:38.091160Z",
          "shell.execute_reply": "2021-08-21T03:03:38.090583Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.016608Z"
        },
        "tags": [],
        "id": "2853e6a5"
      },
      "source": [
        "traindataset = GenderDataset(data['train'])\n",
        "testdataset = GenderDataset(data['test'])\n",
        "\n",
        "# initialize the net, loss and optimizer (SGD)\n",
        "net = Net()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
        "nEpochs, batchSize = 10, 10\n",
        "\n",
        "net = net.to(device)"
      ],
      "id": "2853e6a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79a2b027"
      },
      "source": [
        "Before we jump into the training code, we would first define a function that lets us evaluate how good our model is performing on the test dataset."
      ],
      "id": "79a2b027"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.092096Z",
          "iopub.status.busy": "2021-08-21T03:03:38.091938Z",
          "iopub.status.idle": "2021-08-21T03:03:38.096196Z",
          "shell.execute_reply": "2021-08-21T03:03:38.095662Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.092076Z"
        },
        "tags": [],
        "id": "441d281b"
      },
      "source": [
        "def Evaluate(net, dataset):\n",
        "\tdataloader = DataLoader(dataset, batch_size = batchSize, shuffle=True)\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\twith torch.no_grad():\n",
        "\t\tfor ii, dat in enumerate(dataloader):\n",
        "\t\t\t# unpack the data\n",
        "\t\t\timgs, labels = dat\n",
        "\t\t\tfeats = getFeature(imgs.to(device), vggFace)\n",
        "\t\t\toutput = F.log_softmax(net(feats.to(device)))\n",
        "\t\t\tpredictions = np.argmax(output.detach().cpu(), 1)\n",
        "\t\t\tcorrect += (predictions==labels).sum().item()\n",
        "\t\t\ttotal += len(labels)\n",
        "\treturn correct/total\n",
        "\t\t"
      ],
      "id": "441d281b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02297fea"
      },
      "source": [
        "Let us see the performance of our (un-trained) network on the test set by calling the `evaluate()` function."
      ],
      "id": "02297fea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.097128Z",
          "iopub.status.busy": "2021-08-21T03:03:38.096945Z",
          "iopub.status.idle": "2021-08-21T03:03:38.564742Z",
          "shell.execute_reply": "2021-08-21T03:03:38.564140Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.097109Z"
        },
        "tags": [],
        "id": "72175c81"
      },
      "source": [
        "print(\"accuracy (before training) = \", Evaluate(net, testdataset))"
      ],
      "id": "72175c81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T01:35:32.651622Z",
          "iopub.status.busy": "2021-08-21T01:35:32.651389Z",
          "iopub.status.idle": "2021-08-21T01:35:55.715861Z",
          "shell.execute_reply": "2021-08-21T01:35:55.715324Z",
          "shell.execute_reply.started": "2021-08-21T01:35:32.651598Z"
        },
        "tags": [],
        "id": "931d063e"
      },
      "source": [
        "As you can see, the overall classification accuracies are quite low. At the moment, the network is performing only slightly better than a random assignment of classes. This is to be expected because the parameters of the network are randomly initialized and no training has been done till now.\n",
        "\n",
        "Let us see the code for training the network now."
      ],
      "id": "931d063e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.566275Z",
          "iopub.status.busy": "2021-08-21T03:03:38.565870Z",
          "iopub.status.idle": "2021-08-21T03:03:38.572585Z",
          "shell.execute_reply": "2021-08-21T03:03:38.571829Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.566244Z"
        },
        "tags": [],
        "id": "c5c21fbd"
      },
      "source": [
        "def train(dataset, net, optimizer, criterion, nepochs):\n",
        "\tdataloader = DataLoader(dataset, batch_size = batchSize, shuffle=True)\n",
        "\tfor epoch in range(nepochs):\n",
        "\t\trunning_loss = 0.0\n",
        "\t\tfor ii, (imgs, labels) in enumerate(dataloader):\n",
        "\t\t\twith torch.no_grad(): # we will not be back-propagating this\n",
        "\t\t\t\tfeats = getFeature(imgs.to(device), vggFace)\n",
        "\t\t\toutput = net(feats)\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss = criterion(output, labels.to(device))\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\trunning_loss+=loss.item()\n",
        "\t\tprint(f'epoch {epoch} running_loss : {running_loss/ii}', flush=True)\n",
        "\t\t\n",
        "\t\t\n",
        "\t"
      ],
      "id": "c5c21fbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:38.574815Z",
          "iopub.status.busy": "2021-08-21T03:03:38.574173Z",
          "iopub.status.idle": "2021-08-21T03:03:48.169502Z",
          "shell.execute_reply": "2021-08-21T03:03:48.168476Z",
          "shell.execute_reply.started": "2021-08-21T03:03:38.574748Z"
        },
        "tags": [],
        "id": "24e25c48"
      },
      "source": [
        "train(traindataset, net, optimizer, criterion, nEpochs)"
      ],
      "id": "24e25c48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3436d200"
      },
      "source": [
        "Having trained the network, let us see if the performance on the test set has improved over what we got prior to training. "
      ],
      "id": "3436d200"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:48.175599Z",
          "iopub.status.busy": "2021-08-21T03:03:48.173602Z",
          "iopub.status.idle": "2021-08-21T03:03:48.411154Z",
          "shell.execute_reply": "2021-08-21T03:03:48.410293Z",
          "shell.execute_reply.started": "2021-08-21T03:03:48.175555Z"
        },
        "tags": [],
        "id": "266d0c9f"
      },
      "source": [
        "print(\"accuracy (after training) = \", Evaluate(net, testdataset))"
      ],
      "id": "266d0c9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caf03a74"
      },
      "source": [
        "As you can see, there is a significant jump in performance after training for just 1 epoch (16 iterations).\n",
        "\n",
        "Let us see our network in action as it classifies individual images from the test set. Recall that `label=1` denotes \"male\" whereas `label=0` denotes female."
      ],
      "id": "caf03a74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:48.417205Z",
          "iopub.status.busy": "2021-08-21T03:03:48.415176Z",
          "iopub.status.idle": "2021-08-21T03:03:48.673553Z",
          "shell.execute_reply": "2021-08-21T03:03:48.673105Z",
          "shell.execute_reply.started": "2021-08-21T03:03:48.417165Z"
        },
        "tags": [],
        "id": "e921b030"
      },
      "source": [
        "index = 0\n",
        "imgPath = os.path.join(eroot, data['test']['imgPaths'][index])\n",
        "dispImg = mpimg.imread(imgPath)\n",
        "imgPlot = plt.imshow(dispImg)\n",
        "imgLabel = data['test']['labels'][index]\n",
        "print(\"true label = \", imgLabel)\n",
        "\n",
        "features = getFeature(testdataset[index][0].unsqueeze(0).to(device), vggFace)\n",
        "output = net(features).to(device).data.cpu().numpy()\n",
        "print(\"predicted label = \", np.argmax(output, 1)[0])"
      ],
      "id": "e921b030",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-21T03:03:48.674391Z",
          "iopub.status.busy": "2021-08-21T03:03:48.674232Z",
          "iopub.status.idle": "2021-08-21T03:03:49.007634Z",
          "shell.execute_reply": "2021-08-21T03:03:49.007071Z",
          "shell.execute_reply.started": "2021-08-21T03:03:48.674372Z"
        },
        "tags": [],
        "id": "1576572d"
      },
      "source": [
        "index = 31\n",
        "imgPath = os.path.join(eroot, data['test']['imgPaths'][index])\n",
        "dispImg = mpimg.imread(imgPath)\n",
        "imgPlot = plt.imshow(dispImg)\n",
        "imgLabel = data['test']['labels'][index]\n",
        "print(\"true label = \", imgLabel)\n",
        "\n",
        "features = getFeature(testdataset[index][0].unsqueeze(0).to(device), vggFace)\n",
        "output = net(features).to(device).data.cpu().numpy()\n",
        "print(\"predicted label = \", np.argmax(output, 1)[0])"
      ],
      "id": "1576572d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haJyFRIDhI71"
      },
      "source": [
        ""
      ],
      "id": "haJyFRIDhI71",
      "execution_count": null,
      "outputs": []
    }
  ]
}
