{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "03_FaceVerification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Biometrics/03_FaceVerification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGp70Ue_hiih"
      },
      "source": [
        "### Face Verification\n",
        "In the first tutorial, we saw how the pre-trained VGG-Face network can be used to recognize faces of certain celebrity identities. In facial analysis literature, this is referred to as the \"face identification\" problem. That is, given a face image, face identification can be used to map the input image to a single identity from a pre-defined list of identities. There is another, related problem in this space which is called \"face verification\". Given two face images, the aim of verification is to tell us whether they belong to the same identity or not. On the basis of the definitions, it is easy to see that identification is a multi-class problem whereas verification is a binary classification problem.\n",
        "\n",
        "In this tutorial, we will see how we can use the pre-trained VGG-Face descriptors for the task of face verification on a challenging dataset called  Celebrities in Frontal-Profile in the Wild (CFPW) [1] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T08:00:09.070324Z",
          "iopub.status.busy": "2021-08-20T08:00:09.070121Z",
          "iopub.status.idle": "2021-08-20T08:00:09.075146Z",
          "shell.execute_reply": "2021-08-20T08:00:09.074699Z",
          "shell.execute_reply.started": "2021-08-20T08:00:09.070302Z"
        },
        "tags": [],
        "id": "zWSR9bmJhiim"
      },
      "source": [
        "import cv2, math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline \n",
        "plt.ion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Oy5_G4MPio1v"
      },
      "source": [
        "# download all the data for this notebook.\n",
        "# Warning! This data is hosted on a personal Google drive and may be taken off at any time. \n",
        "#Also, the file is quite big, so please have patience!\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=16O8OmhRn76m8SlIjPNhnwVnWk2djKhak\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T08:47:49.883375Z",
          "iopub.status.busy": "2021-08-20T08:47:49.883171Z",
          "iopub.status.idle": "2021-08-20T08:47:49.886514Z",
          "shell.execute_reply": "2021-08-20T08:47:49.886053Z",
          "shell.execute_reply.started": "2021-08-20T08:47:49.883354Z"
        },
        "tags": [],
        "collapsed": true,
        "id": "b9kzAWH5hiip"
      },
      "source": [
        "root = 'data'\n",
        "eroot = os.path.join(root, 'Experiment_3')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xigi6e7Chiir"
      },
      "source": [
        "You might already be familiar with the following function definitions by now -- `loadImage()` which is used to pre-process the input image for the VGG-Face network and `getVggFeatures()` which returns the fc-layer features from the pre-trained VGG-Face network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:14:36.344206Z",
          "iopub.status.busy": "2021-08-20T09:14:36.344000Z",
          "iopub.status.idle": "2021-08-20T09:14:36.350167Z",
          "shell.execute_reply": "2021-08-20T09:14:36.349658Z",
          "shell.execute_reply.started": "2021-08-20T09:14:36.344184Z"
        },
        "tags": [],
        "id": "yuErhuHxhiit"
      },
      "source": [
        "def loadImage(imgPath):\n",
        "    inputImg = cv2.imread(imgPath)\n",
        "\n",
        "    # re-scale the smaller dim (among width, height) to refSize\n",
        "    refSize, targetSize = 256, 224\n",
        "    imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
        "    if imgCols < imgRows: resizedImg = cv2.resize(inputImg, (refSize, int(refSize * imgRows / imgCols)))\n",
        "    else: resizedImg = cv2.resize(inputImg, (int(refSize * imgCols / imgRows), refSize))\n",
        "\n",
        "    # center-crop\n",
        "    oH, oW = targetSize, targetSize\n",
        "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
        "    anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
        "    croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
        "\n",
        "    # convert shape from (height, width, 3) to (3, width, height)\n",
        "    channel_1, channel_2, channel_3 = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
        "    croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
        "    croppedImg[0], croppedImg[1], croppedImg[2] = channel_1, channel_2, channel_3\n",
        "\n",
        "    # subtract training mean\n",
        "    inputImg = inputImg.astype(float)\n",
        "    trainingMean = [129.1863, 104.7624, 93.5940]\n",
        "    for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]\n",
        "    return croppedImg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:15:07.348878Z",
          "iopub.status.busy": "2021-08-20T09:15:07.348522Z",
          "iopub.status.idle": "2021-08-20T09:15:07.357127Z",
          "shell.execute_reply": "2021-08-20T09:15:07.356641Z",
          "shell.execute_reply.started": "2021-08-20T09:15:07.348855Z"
        },
        "tags": [],
        "id": "DM_iBrA1hiiv"
      },
      "source": [
        "def loadVGGModel( filename):\n",
        "\tdat2 = torch.load(filename)\n",
        "\t# copy dictionary\n",
        "\tif str.split(list(dat2.keys())[0],'.')[0] == 'module':\n",
        "\t\tdat = {}\n",
        "\t\tfor key in dat2.keys():\n",
        "\t\t\tk = '.'.join(str.split(key,'.')[1:])\n",
        "\t\t\tdat[k] = dat2[key]\n",
        "\telse:\n",
        "\t\tdat = dat2\n",
        "\t\t\n",
        "\tn_classes = dat['classifier.6.bias'].shape[0]\n",
        "\tmodel = torchvision.models.vgg16(pretrained = False)\n",
        "\tlastlayer = torch.nn.Linear(in_features = model.classifier[-1].in_features, \\\n",
        "\t\t\t\t\t\t\t   out_features = n_classes, \\\n",
        "\t\t\t\t\t\t\t   bias = True)\n",
        "\tmodel.classifier[-1] = lastlayer\n",
        "\tmodel.load_state_dict(dat)\n",
        "\treturn model\n",
        "\n",
        "def getFeature(x, model):\n",
        "\t# replicate the forward function\n",
        "\tx = model.features(x)\n",
        "\tx = model.avgpool(x)\n",
        "\tx = torch.flatten(x, 1)\n",
        "\t# forward pass only through the first 4 layers of the classifier\n",
        "\tfor ii in range(4):\n",
        "\t\tx = model.classifier[ii](x)\n",
        "\treturn x\n",
        "\n",
        "def getVggFeatures(imgPaths, preTrainedNet):\n",
        "    nImgs = len(imgPaths)\n",
        "    \n",
        "    batchInput = torch.Tensor(nImgs, 3, 224, 224)\n",
        "    for i in range(nImgs): batchInput[i] = torch.from_numpy(loadImage(imgPaths[i]))\n",
        "    \n",
        "    batchOutput = getFeature(batchInput.to(device), preTrainedNet)\n",
        "    return batchOutput.detach().cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ9Mrj2ghiiw"
      },
      "source": [
        "Let us load the dataset and try to visualize the images in the dataset. Since we will be doing verification, the dataset is given in the form of image pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:00:46.520517Z",
          "iopub.status.busy": "2021-08-20T09:00:46.520303Z",
          "iopub.status.idle": "2021-08-20T09:00:46.523816Z",
          "shell.execute_reply": "2021-08-20T09:00:46.523321Z",
          "shell.execute_reply.started": "2021-08-20T09:00:46.520496Z"
        },
        "tags": [],
        "id": "SyRR9lEkhiix"
      },
      "source": [
        "class record():\n",
        "\tdef __init__(self, dat):\n",
        "\t\tself.img1 = str(dat[b'img1'], 'UTF8')\n",
        "\t\tself.img2 = str(dat[b'img2'], 'UTF8')\n",
        "\t\tself.label = dat[b'label']\n",
        "\t\t\n",
        "\tdef __str__(self):\n",
        "\t\treturn f'img1: {self.img1}, img2: {self.img2}, label: {self.label}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:06:09.561063Z",
          "iopub.status.busy": "2021-08-20T09:06:09.560855Z",
          "iopub.status.idle": "2021-08-20T09:06:09.564657Z",
          "shell.execute_reply": "2021-08-20T09:06:09.564160Z",
          "shell.execute_reply.started": "2021-08-20T09:06:09.561041Z"
        },
        "tags": [],
        "id": "6wvjaIq6hiiy"
      },
      "source": [
        "dataset = np.load(eroot + '/cfpw-pairs-dataset.npy', allow_pickle=True)\n",
        "print(\"# image pairs in the dataset = \", len(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v_xf4OZhii0"
      },
      "source": [
        "There are 100 image pairs in the dataset. Each image pair is asociated with a label of either \"+1\" or \"-1\" denoting whether the two images in the pair belong to the same or different individuals respectively.\n",
        "\n",
        "Let us see an example of an image pair where both faces belong to the same identity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:07:46.360092Z",
          "iopub.status.busy": "2021-08-20T09:07:46.359710Z",
          "iopub.status.idle": "2021-08-20T09:07:46.624941Z",
          "shell.execute_reply": "2021-08-20T09:07:46.620124Z",
          "shell.execute_reply.started": "2021-08-20T09:07:46.360068Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "-qFjQ9nfhii1"
      },
      "source": [
        "imgPath1 = os.path.join(eroot, dataset[1].img1)\n",
        "imgPath2 = os.path.join(eroot, dataset[1].img2)\n",
        "dispImg1, dispImg2 = mpimg.imread(imgPath1), mpimg.imread(imgPath2)\n",
        "\n",
        "f, axarr = plt.subplots(1, 2)\n",
        "axarr[0].imshow(dispImg1)\n",
        "axarr[1].imshow(dispImg2)\n",
        "\n",
        "print(\"label = \", dataset[1].label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oLia0f4hii1"
      },
      "source": [
        "Let us also look at some examples of image pairs where the faces belong to different identities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:08:41.095765Z",
          "iopub.status.busy": "2021-08-20T09:08:41.094412Z",
          "iopub.status.idle": "2021-08-20T09:08:41.334641Z",
          "shell.execute_reply": "2021-08-20T09:08:41.334156Z",
          "shell.execute_reply.started": "2021-08-20T09:08:41.095739Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "QZhXG2wuhii2"
      },
      "source": [
        "imgPath1 = os.path.join(eroot, dataset[99].img1)\n",
        "imgPath2 = os.path.join(eroot, dataset[99].img2)\n",
        "dispImg1, dispImg2 = mpimg.imread(imgPath1), mpimg.imread(imgPath2)\n",
        "\n",
        "f, axarr = plt.subplots(1, 2)\n",
        "axarr[0].imshow(dispImg1)\n",
        "axarr[1].imshow(dispImg2)\n",
        "\n",
        "print(\"label = \", dataset[99].label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6zOkomghii3"
      },
      "source": [
        "One aspect among all face pairs in the CFPW dataset is the fact that one of the faces is frontal (looking straight) whereas the other is profile (looking sideways). The presence of such extreme pose variations (frontal-profile) is what makes CFPW a challenging dataset for the task of face verification.\n",
        "\n",
        "Having looked at our dataset, let us define how we are going to compute the (dis)similarity between two faces in a given pair and then subsequently look at some metrics to judge how we are performing on the verification task.\n",
        "\n",
        "Since we will be using the pre-trained VGG-Face descriptors as representations for our faces, we load the pre-trained network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:09:17.058992Z",
          "iopub.status.busy": "2021-08-20T09:09:17.058789Z",
          "iopub.status.idle": "2021-08-20T09:09:22.191525Z",
          "shell.execute_reply": "2021-08-20T09:09:22.191045Z",
          "shell.execute_reply.started": "2021-08-20T09:09:17.058970Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "T_oHHL13hii4"
      },
      "source": [
        "vggFace = loadVGGModel(os.path.join(root, 'VGGFace16.pth'))\n",
        "vggFace = vggFace.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahrg0TpYhii4"
      },
      "source": [
        "Using the above pre-trained network, we will be able to generate the 4096-d representation for each face in the image pair. But, our target task is to determine whether these two faces are the same or not. How do we do that?\n",
        "\n",
        "The intuitive way to do that would be to compute the L2 distance (or any other distance metric of your choice) between the representations of the two faces, and if this distance is less than a certain threshold value, then we can say that the faces belong to the same identity, otherwise not.\n",
        "\n",
        "How do define this threshold? We will come to that later. First, let us set up the code to compute the pair-wise L2 distance between all image pairs in our dataset. As always, we will be iterating through our dataset in units of mini-batches while maintaining a global list of L2 distances between all image pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:16:00.773376Z",
          "iopub.status.busy": "2021-08-20T09:16:00.773171Z",
          "iopub.status.idle": "2021-08-20T09:16:02.009581Z",
          "shell.execute_reply": "2021-08-20T09:16:02.009072Z",
          "shell.execute_reply.started": "2021-08-20T09:16:00.773354Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "XWjAA9MPhii5"
      },
      "source": [
        "nPairs, batchSize = len(dataset), 10\n",
        "classifierScores, labels = [], []\n",
        "\n",
        "for startIdx in range(0, nPairs, batchSize):\n",
        "    endIdx = min(startIdx+batchSize-1, nPairs-1)\n",
        "    size = (endIdx - startIdx + 1)\n",
        "\n",
        "    imgPaths1, imgPaths2, batchLabels = [], [], []\n",
        "    for offset in range(size):\n",
        "        pair = dataset[startIdx+offset]\n",
        "        imgPaths1.append(os.path.join(eroot,  pair.img1))\n",
        "        imgPaths2.append(os.path.join(eroot,  pair.img2))\n",
        "        batchLabels.append(int(pair.label) * -1)\n",
        "    \n",
        "    descrs1 = getVggFeatures(imgPaths1, vggFace).clone()\n",
        "    descrs2 = getVggFeatures(imgPaths2, vggFace).clone()\n",
        "    for i in range(size):\n",
        "        descr1, descr2 = descrs1[i].numpy(), descrs2[i].numpy()\n",
        "        normDescr1, normDescr2 = descr1 / np.linalg.norm(descr1), descr2 / np.linalg.norm(descr2)\n",
        "        classifierScores.append( np.linalg.norm(normDescr1 - normDescr2) )\n",
        "        labels.append(batchLabels[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9oqIJVBhii5"
      },
      "source": [
        "Now that we have the L2 distances and the corresponding labels, let us return to the question of how to chose a threshold and make predictions for face verification. First, let us understand the role played by a threshold. Given a particular threshold for our verification problem, we can classify each image pair whose distance is less than the threshold as \"similar\", and likewise for dissimilar. And having done that, we can compute various metrics such as \"true positive rate\" (tpr) which is the fraction of dissimilar pairs in the dataset which have been correctly classified as being dissimilar. Or, \"false positive rate\" (fpr) which is the fraction of all similar face pairs which have been wrongly classified as dissimilar. All these metrics are dependent on the threshold that we select.\n",
        "\n",
        "However, instead of reporting metrics using a single threshold value, we generally chose several possible different thresholds, compute tpr and fpr for all the different values of threshold and plot them in the form of a graph. This is known as the ROC curve (Region Operating Characteristic curve).\n",
        "\n",
        "After plotting the ROC curve for a binary classifier (such as the one used for face verification), we can select the appropriate threshold value that we wish to \"operate\" in depending upon the level and the kind of errors that we are willing to tolerate.\n",
        "\n",
        "We plot the ROC curve for our verification problem below. Also note that another important metric is the area under the ROC curve (AUC -- Area Under Curve). For a good classifier, the AUC value must be as high as possible (maximum value = 1.0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-20T09:16:17.062486Z",
          "iopub.status.busy": "2021-08-20T09:16:17.062122Z",
          "iopub.status.idle": "2021-08-20T09:16:17.191468Z",
          "shell.execute_reply": "2021-08-20T09:16:17.191032Z",
          "shell.execute_reply.started": "2021-08-20T09:16:17.062463Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "OQO01pXEhii5"
      },
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(labels, classifierScores)\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel(\"False Positive Rate (fpr)\")\n",
        "plt.ylabel(\"True Positive Rate (tpr)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"AUC = \", auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD66QdG9hii6"
      },
      "source": [
        "### Exercices\n",
        "1. Check and compare the face verification accuracies of CFPW dataset with the LFW dataset. Draw both ROC curves in the same plot. For loading the LFW dataset, use the following .t7 file name: lfw-facePairs-dataset.t7 (the rest of the file path remains the same i.e. lfw-facePairs-dataset)\n",
        "\n",
        "2. Use classifier scores and labels to compute the average L2 distance between similar and dissimilar face pairs.\n",
        "\n",
        "3. In the code snippet for computing the L2 distance between the image pairs, why are the labels being flipped? (Notice that we are multiplying each label with -1 before adding it to the list of all labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "n1o5SCc6hii7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}