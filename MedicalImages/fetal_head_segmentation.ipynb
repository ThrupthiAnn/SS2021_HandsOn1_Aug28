{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fetal_head_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThrupthiAnn/SS2021_HandsOn1_Aug28/blob/main/MedicalImages/fetal_head_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdnDOtCa5KEQ"
      },
      "source": [
        "# Segmentation of fetal head from ultrasound images\n",
        "---\n",
        "\n",
        "During pregnancy, the head circumference of the fetus is measured as a quick, noninvasive method to determine if there's a potential problem or if something needs to be monitored. For example, if the head grows too fast, it could sign *hydrocephalus* or water in the brain. If the head grows too slowly, it could be a sign of *microcephaly* or a smaller head than normal.\n",
        "\n",
        "This important biometric is mesured during the ultrasound. While there are computer aided tools for manual measurements on the monitor, there is a required expertise, subjectivity, and room for error. Automated segmentation of fetus head can give quick quantifications relieving the radiologist for other important tasks and decisions.  \n",
        "\n",
        "That's what we will do here! The dataset used for this task can be found at [HC-18 challenge](https://hc18.grand-challenge.org/).\n",
        "\n",
        "## Obtaining data & setting up requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j1yXasiZT3E"
      },
      "source": [
        "# Connect data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUD4FC7jvDL"
      },
      "source": [
        "! pwd\n",
        "! mkdir us_dataset\n",
        "! tar -xzf /content/gdrive/MyDrive/us_data.tar.gz -C us_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwrTSz2ke-1t"
      },
      "source": [
        "# Install kornia, a useful library that does a lot of operations on PyTorch tensors\n",
        "! pip install kornia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_syGMRhgov9x"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm import trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from kornia.losses import dice_loss\n",
        "from kornia.utils import one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pO-dDBO7AHe"
      },
      "source": [
        "!ls us_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtyKjs_l2Ay"
      },
      "source": [
        "from PIL import Image\n",
        "label = Image.open('us_dataset/all_masks/151_HC_mask.png')\n",
        "original = Image.open('us_dataset/all_images/151_HC.png')\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original, cmap='gray')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(label)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM-fOAnmf_ib"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmMRKV3ef9id"
      },
      "source": [
        "\"\"\"Adapted from: https://github.com/meetps/pytorch-semseg\"\"\"\n",
        "\n",
        "class unetConv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm):\n",
        "        super(unetConv2, self).__init__()\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, 3, 1, 0), nn.BatchNorm2d(out_size), nn.ReLU()\n",
        "            )\n",
        "            self.conv2 = nn.Sequential(\n",
        "                nn.Conv2d(out_size, out_size, 3, 1, 0), nn.BatchNorm2d(out_size), nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 3, 1, 0), nn.ReLU())\n",
        "            self.conv2 = nn.Sequential(nn.Conv2d(out_size, out_size, 3, 1, 0), nn.ReLU())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class unetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv):\n",
        "        super(unetUp, self).__init__()\n",
        "        self.conv = unetConv2(in_size, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "        padding = 2 * [offset // 2, offset // 2]\n",
        "        outputs1 = F.pad(inputs1, padding)\n",
        "        return self.conv(torch.cat([outputs1, outputs2], 1))\n",
        "\n",
        "\n",
        "class unet(nn.Module):\n",
        "    def __init__(\n",
        "        self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3, is_batchnorm=True\n",
        "    ):\n",
        "        super(unet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [int(x / self.feature_scale) for x in filters]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.center = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat4 = unetUp(filters[4], filters[3], self.is_deconv)\n",
        "        self.up_concat3 = unetUp(filters[3], filters[2], self.is_deconv)\n",
        "        self.up_concat2 = unetUp(filters[2], filters[1], self.is_deconv)\n",
        "        self.up_concat1 = unetUp(filters[1], filters[0], self.is_deconv)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final = nn.Conv2d(filters[0], n_classes, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        conv1 = self.conv1(inputs)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "        conv4 = self.conv4(maxpool3)\n",
        "        maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "        center = self.center(maxpool4)\n",
        "        up4 = self.up_concat4(conv4, center)\n",
        "        up3 = self.up_concat3(conv3, up4)\n",
        "        up2 = self.up_concat2(conv2, up3)\n",
        "        up1 = self.up_concat1(conv1, up2)\n",
        "\n",
        "        final = self.final(up1)\n",
        "\n",
        "        return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npcVLQEl-aJ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0lSQbSAlFlk"
      },
      "source": [
        "class ImageLoader(Dataset):\n",
        "  \"\"\" Data loader class \"\"\"\n",
        "\n",
        "  def __init__(self, path, file_list, aug_list=None, aug_prob=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      path (str): path where images stored\n",
        "      file_list (List[str]): list of images in current split\n",
        "      aug_list (List[str]): list of torchvision transforms\n",
        "      aug_prob (float): Probability of applying random aug (if aug_list != None)\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.file_list = file_list\n",
        "    self.aug_list = aug_list\n",
        "    self.aug_prob = aug_prob\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_list)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" Preprocess and return a single sample & label \"\"\"\n",
        "    img_name = os.path.join(self.path, 'all_images', self.file_list[idx])\n",
        "    mask_fname = self.file_list[idx].split('.')[0] + '_mask.png'\n",
        "    mask_name = os.path.join(self.path, 'all_masks', mask_fname)\n",
        "    img = Image.open(img_name)\n",
        "    mask = Image.open(mask_name)\n",
        "\n",
        "    # Resize to dimensions supported by Vanilla UNet\n",
        "    img = img.resize((572, 572), Image.LANCZOS)\n",
        "    mask = mask.resize((388, 388), Image.NEAREST)\n",
        "\n",
        "    img = np.array(img)\n",
        "    mask = np.array(mask)\n",
        "    mask[mask == 255] = 1\n",
        "\n",
        "    mask = torch.Tensor([mask])\n",
        "\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  \n",
        "    img = torch.Tensor(img)\n",
        "    img = img.permute(2, 0, 1)\n",
        "\n",
        "    # select and apply random augmentation (if passed)\n",
        "    if self.aug_list:\n",
        "      do_aug = np.random.choice([True, False], 1, p=[self.aug_prob,\n",
        "                                                     1-self.aug_prob])\n",
        "      if do_aug:\n",
        "        aug_name = np.random.choice(self.aug_list, 1)\n",
        "        img = aug_name[0](img)\n",
        "    img = (img - torch.mean(img)) / torch.std(img)\n",
        "    return img, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pU9_SjM0pax"
      },
      "source": [
        "def get_data_loaders(categories, path, file_lists,\n",
        "                     augment, aug_prob, batch_size):\n",
        "  \"\"\"\n",
        "  Wrapper function to return dataloader(s)\n",
        "  Args:\n",
        "    categories (List[str]): names of processes for which dataloader needed\n",
        "    path (str): path where images stored\n",
        "    file_lists (List[List[str]]): list of file lists\n",
        "    augment (boolean): whether to apply augmentation\n",
        "    aug_prob (float): Probability of applying random aug\n",
        "    batch_size (int): batch size\n",
        "  Returns:\n",
        "    torch.utils.data.DataLoader object\n",
        "  \"\"\"\n",
        "  loaders = []\n",
        "  for i, category in enumerate(categories):\n",
        "    if category == 'train' and augment:\n",
        "      aug_list = [\n",
        "          transforms.RandomAffine(0, translate=(0.2, 0.2)),\n",
        "          transforms.RandomHorizontalFlip(p=1),\n",
        "          transforms.RandomRotation(degrees=(-10, 10), fill=(0,)),\n",
        "          transforms.GaussianBlur((17, 17), (11, 11))\n",
        "      ]\n",
        "    else:\n",
        "      aug_list = None\n",
        "    loader = DataLoader(\n",
        "        ImageLoader(path, file_lists[i], aug_list, aug_prob),\n",
        "        batch_size,\n",
        "        num_workers=1\n",
        "        )\n",
        "    loaders.append(loader)\n",
        "  return loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPhL86aF4sjb"
      },
      "source": [
        "## Train/val/test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihzXgpzJnz9b"
      },
      "source": [
        "# dice score\n",
        "def integral_dice(pred, gt, k):\n",
        "    '''\n",
        "    Dice coefficient for multiclass hard thresholded prediction consisting of \n",
        "    integers instead of binary values. \n",
        "    k = integer for class for which Dice is being calculated.\n",
        "    '''\n",
        "    return (torch.sum(pred[gt == k] == k)*2.0\n",
        "            / (torch.sum(pred[pred == k] == k)\n",
        "               + torch.sum(gt[gt == k] == k)).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLnI2nmj2XZK"
      },
      "source": [
        "def learn(model, loader, optimizer, process):\n",
        "  \"\"\" main function for single epoch of train, val or test \"\"\"\n",
        "  dice_list = []\n",
        "  running_loss = 0\n",
        "  num_batches = len(loader)\n",
        "  with trange(num_batches, desc=process, ncols=100) as t:\n",
        "    for batch_num, sample in enumerate(loader):\n",
        "      img_batch, masks = sample\n",
        "      masks = masks[:, 0, :, :, 0].long()\n",
        "      # one hot encoding labels\n",
        "      masks_oh = one_hot(masks, num_classes=2, device='cpu', dtype=masks.dtype)\n",
        "      if process == 'train':\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = F.softmax(model(img_batch.cuda()), 1)\n",
        "        loss = F.binary_cross_entropy(preds, masks_oh.cuda())\n",
        "        # loss = dice_loss(preds, masks.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          preds = F.softmax(model(img_batch.cuda()), 1)\n",
        "          loss = F.binary_cross_entropy(preds, masks_oh.cuda())\n",
        "          # loss = dice_loss(preds, masks.cuda())\n",
        "      hard_preds = torch.argmax(preds, 1)\n",
        "      dice = integral_dice(hard_preds, masks, 1)\n",
        "      dice_list.append(dice.item())\n",
        "      running_loss += loss  \n",
        "      t.set_postfix(loss=running_loss.item()/(float(batch_num+1)*batch_size))\n",
        "      t.update()\n",
        "  mean_dice = np.mean(np.array(dice_list))\n",
        "  final_loss = running_loss.item()/(num_batches*batch_size)\n",
        "  return mean_dice, final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QjMxW-3C41l"
      },
      "source": [
        "def get_splits(all_names, train_size, val_size, test_size):\n",
        "  split1_size = (val_size+test_size)\n",
        "  split2_size = test_size / (val_size+test_size)\n",
        "  trn_names, valtst_names = train_test_split(\n",
        "      all_names, test_size=split1_size, random_state=0)\n",
        "  val_names, tst_names = train_test_split(\n",
        "      valtst_names, test_size=split2_size, random_state=0)\n",
        "  return trn_names, val_names, tst_names "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SsSa05k__2U"
      },
      "source": [
        "def perform_learning(model, optimizer, path, all_names, batch_size,\n",
        "                     splits, num_epochs):\n",
        "  \"\"\" Wrapper function to run train, val, test loops \"\"\"\n",
        "  train_size, val_size, test_size = splits\n",
        "  trn_names, val_names, tst_names = get_splits(all_names, train_size, val_size,\n",
        "                                               test_size)\n",
        "  train_loader, val_loader, test_loader = get_data_loaders(\n",
        "      ['train', 'val', 'test'],\n",
        "      path, [trn_names, val_names, tst_names],\n",
        "      augment=True,\n",
        "      aug_prob=0.5,\n",
        "      batch_size=batch_size\n",
        "      )\n",
        "  for epoch_num in range(num_epochs):\n",
        "    train_dice, train_loss = learn(model, train_loader, optimizer, 'train')\n",
        "    print(f'Training Epoch {epoch_num} - Loss: {train_loss} ; Dice : {train_dice}')\n",
        "    val_dice, val_loss = learn(model, val_loader, optimizer, 'val')\n",
        "    print(f'Validation Epoch {epoch_num} - Loss: {val_loss} ; Dice : {val_dice}')\n",
        "  tst_dice, tst_loss = learn(model, test_loader, optimizer, 'test')\n",
        "  print(f'Test - Loss: {tst_loss} ; Dice : {tst_dice}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4llZPlvHbxa"
      },
      "source": [
        "**Training**\n",
        "\n",
        "Metric used: Dice score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WMbMigmHVhn"
      },
      "source": [
        "path = '/content/us_dataset'\n",
        "all_names = os.listdir(os.path.join(path, 'all_images'))\n",
        "\n",
        "lr = 1e-4\n",
        "wt_dec = 1e-4\n",
        "num_epochs = 5\n",
        "batch_size = 2\n",
        "splits = [0.8, 0.1, 0.1]\n",
        "\n",
        "model = unet(n_classes=2)\n",
        "model = model.cuda()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=wt_dec)\n",
        "\n",
        "perform_learning(model, optimizer, path, all_names, batch_size,\n",
        "                 splits, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J5dSLXkXCVs"
      },
      "source": [
        "**Qualitative Assessment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owro90xgJyXi"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/us_seg.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hySi7RIRigTS"
      },
      "source": [
        "# Looking at outputs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = '/content/us_dataset'\n",
        "all_names = os.listdir(os.path.join(path, 'all_images'))\n",
        "\n",
        "splits = [0.8, 0.1, 0.1]\n",
        "train_size, val_size, test_size = splits\n",
        "trn_names, val_names, tst_names = get_splits(all_names, train_size, val_size,\n",
        "                                              test_size)\n",
        "_, _, test_loader = get_data_loaders(\n",
        "    ['train', 'val', 'test'],\n",
        "    path, [trn_names, val_names, tst_names],\n",
        "    augment=True,\n",
        "    aug_prob=0.5,\n",
        "    batch_size=1\n",
        "    )\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for batch_num, sample in enumerate(test_loader):\n",
        "  X, label = sample\n",
        "\n",
        "  preds = F.softmax(model(X.cuda()), 1) \n",
        "  hard_preds = torch.argmax(preds, 1)\n",
        "  mask = hard_preds[0].detach().cpu().numpy()\n",
        "  img = X[0].permute(1, 2, 0).numpy()\n",
        "  img = img.astype('float')\n",
        "  img = cv2.resize(img, (388, 388))\n",
        "  img = (img - np.max(img)) / (np.max(img) - np.min(img))\n",
        "\n",
        "  plt.figure()\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow((img*255).astype('uint8'), cmap='gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow((img*255).astype('uint8'), cmap='gray')\n",
        "  plt.imshow(mask, cmap='jet', alpha=0.3)  \n",
        "\n",
        "\n",
        "#   plt.title('Overlayed Attributions')\n",
        "  plt.axis('off')\n",
        "#   plt.colorbar()\n",
        "  plt.show()                 \n",
        "  cnt += 1\n",
        "  if cnt >= 2:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUmWnqfQ-F9Q"
      },
      "source": [
        "# **Excercise**\n",
        "\n",
        "\n",
        "---\n",
        "*   Apply ellipse fitting on the segmentation outputs (use OpenCV)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcC1VGl277Bt"
      },
      "source": [
        "# **Resources**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "For other interesting applications and data, check out [grand challenge](https://grand-challenge.org/challenges/). "
      ]
    }
  ]
}